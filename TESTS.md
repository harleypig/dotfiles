# Testing Framework and Strategy

**Version:** v1.0.0

## Purpose

This document defines the testing framework, strategy, and requirements for
the dotfiles repository. It specifies how to write, organize, run, and
maintain tests for shell scripts, libraries, and configurations.

This document conforms to the agent-consumed document schema defined in
`AGENTS.md`. For general workflow and development guidelines, see
`WORKFLOW.md`.

**Precedence hierarchy:** `TESTS.md` > `WORKFLOW.md` > `AGENTS.md`

Testing-specific rules in this document override general principles in both
`WORKFLOW.md` and `AGENTS.md` for test-related operations.

## Testing Framework

### Primary Framework: BATS

**BATS (Bash Automated Testing System)** is the primary testing framework for
this repository.

* **Website:** https://github.com/bats-core/bats-core
* **Version:** bats-core 1.x or later recommended
* **Test file extension:** `.bats`
* **Test location:** `tests/` directory

### Test Organization

```
tests/
├── fixtures/               # Test fixtures and sample data
├── helpers/               # Helper functions and utilities for tests
├── templates/             # Test templates
├── build-meta-tests       # Script to generate meta-tests
├── test_*.bats           # Individual test files
└── meta_*.bats           # Auto-generated meta-tests
```

**Naming conventions:**

* Unit tests: `test_<component>.bats` (e.g., `test_cleanpath.bats`)
* Integration tests: `test_integration_<feature>.bats`
* Meta-tests: `meta_<check>.bats` (auto-generated)
* Helper files: `helper_<name>.bash` in `tests/helpers/`

## Test Types

### 1. Meta-Tests (Automated Quality Checks)

**Purpose:** Automated validation of code quality and structure.

**Generated by:** `tests/build-meta-tests` script

**Examples:**

* Bash syntax validation (`bash -n`)
* Shebang validation
* Symbolic link validation
* File permission checks

**Meta-tests MUST:**

* Be auto-generated from repository structure
* Run quickly (< 1 second per test)
* Not require external dependencies beyond bash/coreutils
* Be regenerated when repository structure changes

**Creating meta-tests:**

```bash
cd tests
./build-meta-tests
```

### 2. Unit Tests (Individual Component Testing)

**Purpose:** Test individual scripts, functions, or libraries in isolation.

**Scope:**

* Single script in `bin/`
* Single library in `lib/`
* Single function or configuration module

**Unit tests MUST:**

* Test both success and failure paths
* Mock external dependencies
* Be independent (no shared state between tests)
* Clean up any created files/directories

**Example structure:**

```bash
#!/usr/bin/env bats

# tests/test_cleanpath.bats

setup() {
  # Run before each test
  load helpers/common
  export PATH="$BATS_TEST_DIRNAME/../bin:$PATH"
}

teardown() {
  # Run after each test
  cleanup_temp_files
}

@test "cleanpath removes duplicate entries" {
  result=$(echo "/bin:/usr/bin:/bin" | cleanpath)
  [ "$result" = "/bin:/usr/bin" ]
}

@test "cleanpath handles empty input" {
  result=$(echo "" | cleanpath)
  [ "$result" = "" ]
}

@test "cleanpath fails on invalid input" {
  run cleanpath <<< "not:a:valid:path"
  [ "$status" -ne 0 ]
}
```

### 3. Integration Tests (Multi-Component Testing)

**Purpose:** Test interactions between multiple components.

**Scope:**

* Shell startup sequence (`shell-startup` loading all modules)
* Configuration interactions (e.g., PATH building with multiple tools)
* Script workflows (multi-step operations)

**Integration tests MUST:**

* Test realistic usage scenarios
* Verify component interactions
* Test error propagation
* Clean up thoroughly

**Example:**

```bash
@test "shell-startup loads all modules without errors" {
  run bash -c "source $BATS_TEST_DIRNAME/../shell-startup 2>&1"
  [ "$status" -eq 0 ]
  [[ ! "$output" =~ "error" ]]
  [[ ! "$output" =~ "Error" ]]
}
```

### 4. Completion Tests (Bash Completion Validation)

**Purpose:** Verify bash completion functionality.

**Scope:**

* Completion files in `config/bash-completion/`
* Generated completions
* Completion edge cases

**Completion tests SHOULD:**

* Load completion file without errors
* Verify completion function exists
* Test basic completion scenarios (when feasible)

**Note:** Full completion testing is complex; focus on loading and basic
validation.

## Test Requirements

### For New Code

**All new scripts in `bin/` MUST have:**

* Unit tests covering primary functionality
* Error handling tests
* Edge case tests (empty input, invalid input, etc.)

**All new libraries in `lib/` MUST have:**

* Unit tests for each exported function
* Tests verifying library can be sourced without errors
* Tests for error conditions

**All new shell-startup modules in `config/shell-startup/` SHOULD have:**

* Integration test verifying module loads without errors
* Tests for conditional logic (if applicable)

### For Modified Code

**When modifying existing code:**

* Existing tests MUST still pass
* New functionality MUST include new tests
* Bug fixes MUST include regression tests

### Test Coverage Goals

**Phase-based approach (incremental):**

* **Phase 1 (Current):** Meta-tests + critical script tests
* **Phase 2 (Near-term):** Core bin/ scripts, key lib/ libraries
* **Phase 3 (Long-term):** Comprehensive coverage for all components

**Priority for test coverage:**

1. **Critical scripts:** Scripts that can cause data loss or system issues
2. **Core libraries:** Libraries used by multiple components
3. **Complex logic:** Scripts with conditional logic, loops, parsing
4. **Simple utilities:** Simple wrapper scripts (lower priority)

## Running Tests

### Run All Tests

```bash
cd ~/dotfiles
bats tests/
```

### Run Specific Test File

```bash
bats tests/test_cleanpath.bats
```

### Run Tests Matching Pattern

```bash
bats tests/test_*.bats        # All unit tests
bats tests/meta_*.bats        # All meta-tests
bats tests/test_integration_*.bats  # All integration tests
```

### Run Single Test

```bash
bats tests/test_cleanpath.bats --filter "removes duplicate"
```

### Verbose Output

```bash
bats -t tests/                # Tap output
bats --verbose-run tests/     # Verbose
```

## Writing Tests

### Test Structure

```bash
#!/usr/bin/env bats

# Load helpers if needed
setup() {
  load helpers/common
  # Per-test setup
}

teardown() {
  # Per-test cleanup
}

setup_file() {
  # One-time setup for entire file
}

teardown_file() {
  # One-time cleanup for entire file
}

@test "descriptive test name" {
  # Test code here
  run command arg1 arg2
  [ "$status" -eq 0 ]
  [ "$output" = "expected" ]
}
```

### Common Patterns

**Testing exit status:**

```bash
run command args
[ "$status" -eq 0 ]           # Success
[ "$status" -ne 0 ]           # Failure
[ "$status" -eq 2 ]           # Specific exit code
```

**Testing output:**

```bash
run command args
[ "$output" = "exact match" ]
[[ "$output" =~ "regex pattern" ]]
[[ "$output" =~ ^expected.* ]]
```

**Testing stderr:**

```bash
run command args
[[ "$stderr" =~ "error message" ]]
```

**Testing files:**

```bash
[ -f "/path/to/file" ]        # File exists
[ -d "/path/to/dir" ]         # Directory exists
[ -x "/path/to/script" ]      # File is executable
```

**Using fixtures:**

```bash
cp tests/fixtures/sample.conf "$BATS_TEST_TMPDIR/test.conf"
run command --config "$BATS_TEST_TMPDIR/test.conf"
```

### Mocking External Commands

**Simple mock:**

```bash
mock_git() {
  echo "mocked output"
  return 0
}

# In test:
git() { mock_git "$@"; }
run some_script_that_calls_git
```

**Conditional mock:**

```bash
mock_git() {
  case "$1" in
    status) echo "On branch master" ;;
    branch) echo "* master" ;;
    *) return 1 ;;
  esac
}
```

### Helper Functions

Place commonly used test utilities in `tests/helpers/common.bash`:

```bash
# tests/helpers/common.bash

# Load in tests with: load helpers/common

cleanup_temp_files() {
  rm -rf "$BATS_TEST_TMPDIR"/*
}

assert_file_exists() {
  [ -f "$1" ] || fail "File does not exist: $1"
}

assert_output_contains() {
  [[ "$output" =~ $1 ]] || fail "Output does not contain: $1"
}
```

## CI Integration

### GitHub Actions

Tests MUST run on:

* Every push to `master`
* Every pull request
* Manual workflow dispatch

**Workflow requirements:**

* Run all BATS tests
* Report results as job status
* Fail workflow if any test fails
* Display test output on failure

**See `.github/workflows/ci.yml` for implementation.**

### Pre-commit Integration

**Meta-tests MAY run as pre-commit hooks** for fast feedback:

* Bash syntax validation
* Shellcheck integration
* Basic structural checks

**Full test suite runs in CI** (too slow for pre-commit).

## Test Development Workflow

### Adding Tests for New Feature

1. Write tests first (TDD approach recommended)
2. Write implementation
3. Run tests locally: `bats tests/test_<feature>.bats`
4. Fix until tests pass
5. Run full test suite: `bats tests/`
6. Commit with tests included

### Adding Tests for Bug Fix

1. Write test that reproduces bug (test should fail)
2. Fix bug
3. Verify test now passes
4. Run full test suite
5. Commit with test and fix together

### Regenerating Meta-Tests

When adding new scripts or changing repository structure:

```bash
cd tests
./build-meta-tests
git add meta_*.bats
git commit -m "Regenerate meta-tests"
```

## Test Maintenance

### Regular Tasks

* Run full test suite before pushing: `bats tests/`
* Regenerate meta-tests when adding/removing files
* Review and update failing tests when changing behavior
* Add tests for reported bugs
* Keep helper functions DRY

### When Tests Fail

1. **Understand the failure:** Read test output carefully
2. **Check if behavior changed intentionally:** Update test if needed
3. **Check if test is flaky:** Fix flaky tests immediately
4. **Never ignore failing tests:** Fix or update, don't skip

### Test Quality Standards

**Good tests are:**

* **Fast:** Run in milliseconds, not seconds
* **Isolated:** No dependencies on other tests
* **Repeatable:** Same result every time
* **Self-contained:** All setup/teardown in test file
* **Clear:** Obvious what is being tested

**Avoid:**

* Tests that depend on execution order
* Tests that require manual setup
* Tests that modify global system state
* Tests with hardcoded absolute paths
* Tests that make network requests (unless mocked)

## Troubleshooting

### Tests Fail Locally But Pass in CI (or vice versa)

* Check environment differences (PATH, installed tools, etc.)
* Verify BATS version matches
* Check for platform-specific behavior (Linux vs macOS)
* Look for hardcoded paths

### Tests Are Slow

* Profile with `time bats tests/`
* Mock expensive operations (network, disk I/O)
* Use `setup_file` for expensive one-time setup
* Consider splitting into fast/slow test suites

### Cannot Mock External Command

* Redefine function in test: `command() { mock; }`
* Use stub helper functions
* Inject dependencies (pass command as argument)
* Consider refactoring code to be more testable

## Future Enhancements

**Planned improvements:**

* Coverage reporting (bash coverage tools)
* Performance benchmarking tests
* Automated test generation from inline examples
* Test result history tracking
* Parallel test execution for large test suites

## Resources

* **BATS documentation:** https://bats-core.readthedocs.io/
* **BATS repository:** https://github.com/bats-core/bats-core
* **Test examples:** See `tests/` directory in this repository
* **Helper libraries:** https://github.com/bats-core/bats-support
  and https://github.com/bats-core/bats-assert

## Questions and Issues

* For general development workflow, see `WORKFLOW.md`
* For agent behavior, see `AGENTS.md`
* For test examples, examine existing tests in `tests/`
* For bugs or suggestions, open a GitHub issue
